21/12/07 19:00:51 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/12/07 19:00:51 INFO SecurityManager - Changing view acls to: 1000650000
21/12/07 19:00:51 INFO SecurityManager - Changing modify acls to: 1000650000
21/12/07 19:00:51 INFO SecurityManager - Changing view acls groups to: 
21/12/07 19:00:51 INFO SecurityManager - Changing modify acls groups to: 
21/12/07 19:00:51 INFO SecurityManager - SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(1000650000); groups with view permissions: Set(); users  with modify permissions: Set(1000650000); groups with modify permissions: Set()
2021-12-07 19:00:52.175+0000 INFO: Job run ID: de312f9d-5aa8-4ce4-99b4-5c214835f019
2021-12-07 19:00:52.192+0000 INFO: Loading libraries.
2021-12-07 19:00:52.222+0000 INFO: Running jsonlite v 1.7.2
2021-12-07 19:00:52.223+0000 INFO: Running dplyr v 0.8.2
2021-12-07 19:00:54.266+0000 INFO: Runtime environment: Spark
2021-12-07 19:00:54.267+0000 INFO: Initializing parameters for Data Refinery flow.
Spark package found in SPARK_HOME: /opt/ibm/spark
21/12/07 19:00:55 INFO SparkContext - Running Spark version 3.0.2
21/12/07 19:00:55 INFO ResourceUtils - ==============================================================
21/12/07 19:00:55 INFO ResourceUtils - Resources for spark.driver:

21/12/07 19:00:55 INFO ResourceUtils - ==============================================================
21/12/07 19:00:55 INFO SparkContext - Submitted application: SparkR
21/12/07 19:00:55 INFO SecurityManager - Changing view acls to: 1000650000
21/12/07 19:00:55 INFO SecurityManager - Changing modify acls to: 1000650000
21/12/07 19:00:55 INFO SecurityManager - Changing view acls groups to: 
21/12/07 19:00:55 INFO SecurityManager - Changing modify acls groups to: 
21/12/07 19:00:55 INFO SecurityManager - SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(1000650000); groups with view permissions: Set(); users  with modify permissions: Set(1000650000); groups with modify permissions: Set()
21/12/07 19:00:55 INFO Utils - Successfully started service 'sparkDriver' on port 40719.
21/12/07 19:00:55 INFO SparkEnv - Registering MapOutputTracker
21/12/07 19:00:55 INFO SparkEnv - Registering BlockManagerMaster
21/12/07 19:00:55 INFO BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/12/07 19:00:55 INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
21/12/07 19:00:55 INFO SparkEnv - Registering BlockManagerMasterHeartbeat
21/12/07 19:00:55 INFO DiskBlockManager - Created local directory at /tmp/spark/scratch/blockmgr-7b0844e9-93ce-44f7-8af8-60d15c53ca73
21/12/07 19:00:55 INFO MemoryStore - MemoryStore started with capacity 2.2 GiB
21/12/07 19:00:55 INFO SparkEnv - Registering OutputCommitCoordinator
21/12/07 19:00:56 INFO Utils - Successfully started service 'SparkUI' on port 4040.
21/12/07 19:00:56 INFO SparkUI - Bound SparkUI to 0.0.0.0, and started at http://spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:4040
21/12/07 19:00:56 INFO SparkContext - Added file file:/opt/ibm/third-party/libs/exec/Runner3.R at spark://spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:40719/files/Runner3.R with timestamp 1638903655140
21/12/07 19:00:56 INFO Utils - Copying /opt/ibm/third-party/libs/exec/Runner3.R to /tmp/spark/scratch/spark-938f3a94-de1d-4bb1-a3da-d62eac18e2ac/userFiles-da04895b-21d6-43c0-9ddf-3bedc1603eb9/Runner3.R
21/12/07 19:00:57 INFO StandaloneAppClient$ClientEndpoint - Connecting to master spark://10.254.25.35:7077...
21/12/07 19:00:57 INFO TransportClientFactory - Successfully created connection to /10.254.25.35:7077 after 370 ms (286 ms spent in bootstraps)
21/12/07 19:00:57 INFO StandaloneSchedulerBackend - Connected to Spark cluster with app ID app-20211207190057-0000
21/12/07 19:00:57 INFO Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41001.
21/12/07 19:00:57 INFO NettyBlockTransferService - Server created on spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:41001
21/12/07 19:00:57 INFO BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/12/07 19:00:57 INFO StandaloneAppClient$ClientEndpoint - Executor added: app-20211207190057-0000/0 on worker-20211207190046-10.254.20.114-43217 (10.254.20.114:43217) with 1 core(s)
21/12/07 19:00:58 INFO BlockManagerMaster - Registering BlockManager BlockManagerId(driver, spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019, 41001, None)
21/12/07 19:00:58 INFO StandaloneSchedulerBackend - Granted executor ID app-20211207190057-0000/0 on hostPort 10.254.20.114:43217 with 1 core(s), 4.0 GiB RAM
21/12/07 19:00:58 INFO StandaloneAppClient$ClientEndpoint - Executor added: app-20211207190057-0000/1 on worker-20211207190044-10.254.32.196-42955 (10.254.32.196:42955) with 1 core(s)
21/12/07 19:00:58 INFO StandaloneSchedulerBackend - Granted executor ID app-20211207190057-0000/1 on hostPort 10.254.32.196:42955 with 1 core(s), 4.0 GiB RAM
21/12/07 19:00:58 INFO BlockManagerMasterEndpoint - Registering block manager spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:41001 with 2.2 GiB RAM, BlockManagerId(driver, spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019, 41001, None)
21/12/07 19:00:58 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019, 41001, None)
21/12/07 19:00:58 INFO BlockManager - Initialized BlockManager: BlockManagerId(driver, spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019, 41001, None)
21/12/07 19:00:58 INFO StandaloneAppClient$ClientEndpoint - Executor updated: app-20211207190057-0000/1 is now RUNNING
21/12/07 19:00:58 INFO StandaloneAppClient$ClientEndpoint - Executor updated: app-20211207190057-0000/0 is now RUNNING
21/12/07 19:00:58 INFO SingleEventLogFileWriter - Logging events to file:/home/spark/spark-events/app-20211207190057-0000.inprogress
21/12/07 19:00:58 INFO StandaloneSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
21/12/07 19:00:59 INFO HiveConf - Found configuration file null
21/12/07 19:00:59 INFO SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/spark/shared/spark-warehouse').
21/12/07 19:00:59 INFO SharedState - Warehouse path is 'file:/home/spark/shared/spark-warehouse'.
21/12/07 19:01:00 INFO SparkUI - Stopped Spark web UI at http://spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:4040
21/12/07 19:01:01 INFO StandaloneSchedulerBackend - Shutting down all executors
21/12/07 19:01:01 INFO CoarseGrainedSchedulerBackend$DriverEndpoint - Asking each executor to shut down
21/12/07 19:01:01 INFO MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
21/12/07 19:01:01 INFO MemoryStore - MemoryStore cleared
21/12/07 19:01:01 INFO BlockManager - BlockManager stopped
21/12/07 19:01:01 INFO BlockManagerMaster - BlockManagerMaster stopped
21/12/07 19:01:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
21/12/07 19:01:01 INFO SparkContext - Successfully stopped SparkContext
2021-12-07 19:01:01.573+0000 INFO: Set additional spark config properties.
2021-12-07 19:01:01.573+0000 INFO: Spark master: spark://spark-master-headless-de312f9d-5aa8-4ce4-99b4-5c214835f019:7077
2021-12-07 19:01:13.831+0000 INFO: Connected to Spark runtime environment.
2021-12-07 19:01:13.832+0000 INFO: R version 3.6.3 (2020-02-29)
2021-12-07 19:01:13.832+0000 INFO: Running Spark version 3.0.2
2021-12-07 19:01:13.835+0000 INFO: Running Sparklyr version 1.5.2
2021-12-07 19:01:13.950+0000 INFO: Running Arrow version 5.0.0
2021-12-07 19:01:13.951+0000 INFO: Registering UDFs.
2021-12-07 19:01:15.108+0000 INFO: Completed processing user info from authentication token.
2021-12-07 19:01:15.108+0000 INFO: Using service token for flow run.
2021-12-07 19:01:15.217+0000 INFO: Running ShaperFlowPkg version 5.2.3 (IMAGE:4.0.1049)
2021-12-07 19:01:15.217+0000 INFO: Running Connector version 5.0.288
2021-12-07 19:01:15.217+0000 INFO: Running UDF version 7.0.45
2021-12-07 19:01:15.217+0000 INFO: Number of partitions: 2
2021-12-07 19:01:15.390+0000 INFO: Reading Data Refinery flow: USStatesFull.csv_flow
2021-12-07 19:01:15.391+0000 INFO: Creating Spark payload: Processing source and target properties, and operations.
2021-12-07 19:01:15.995+0000 INFO: Completed reading properties for 'source1': USStatesFull.csv
2021-12-07 19:01:15.995+0000 INFO: Data source type: assetfiles
2021-12-07 19:01:15.996+0000 INFO: AvailMemBytes: 2576979205 Size: 1172 Cache: TRUE
2021-12-07 19:01:30.291+0000 WARNING: Data asset name 'USStatesFull_csv_shaped' already exists. Data asset will be overwritten.
2021-12-07 19:01:30.633+0000 INFO: Completed reading properties for 'target1': USStatesFull_csv_shaped
2021-12-07 19:01:30.633+0000 INFO: Data source type: assetfiles
2021-12-07 19:01:30.633+0000 INFO: Running Spark payload.
2021-12-07 19:01:53.012+0000 INFO: Time taken to run spark payload: 22 secs
2021-12-07 19:01:53.129+0000 INFO: Total execution time : 38 secs
2021-12-07 19:01:53.129+0000 INFO: Updating job run status to 'FINISHED'.
2021-12-07 19:01:53.130+0000 INFO: 50 rows read.
2021-12-07 19:01:53.130+0000 INFO: 939 bytes read.
2021-12-07 19:01:53.130+0000 INFO: 50 rows written.
2021-12-07 19:01:53.130+0000 INFO: 939 bytes written.
Warning messages:
1: replacing previous import ‘lubridate::union’ by ‘dplyr::union’ when loading ‘ShaperFlowPkg’ 
2: replacing previous import ‘lubridate::intersect’ by ‘dplyr::intersect’ when loading ‘ShaperFlowPkg’ 
3: replacing previous import ‘lubridate::setdiff’ by ‘dplyr::setdiff’ when loading ‘ShaperFlowPkg’ 
4: package ‘SparkR’ was built under R version 4.0.3 
21/12/07 19:01:56 INFO ShutdownHookManager - Shutdown hook called
21/12/07 19:01:56 INFO ShutdownHookManager - Deleting directory /tmp/spark-e546fc9d-ff76-4f57-bb41-f69457c11de5
2021-12-07 19:01:56,442 Thread-4 WARN Unable to register Log4j shutdown hook because JVM is shutting down. Using SimpleLogger
